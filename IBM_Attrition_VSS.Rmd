---
title: "IBM HR Analytics Employee Attrition & Performance"
author: "Rohan's Four - Rohan Jain, Ali Shahid, Sehrish Saud, Julian Ramirez"
output:
  html_document:
    css: ../AnalyticsStyles/default.css
    theme: paper
    toc: yes
    toc_float:
      collapsed: no
      smooth_scroll: yes
  pdf_document:
    includes:
      in_header: ../AnalyticsStyles/default.sty
always_allow_html: yes
---

```{r echo=FALSE, message=FALSE}
make_pdf_file = 0 # SET THIS TO 1 IF WE COMPILE PDF FILE, 0 OTHERWISE (FOR HTML)

source("../AnalyticsLibraries/library.R")
source("../AnalyticsLibraries/heatmapOutput.R")
# Package options
#library("fifer")
ggthemr('fresh')  # ggplot theme
opts_knit$set(progress=FALSE, verbose=FALSE)
opts_chunk$set(echo=FALSE, fig.align="center", fig.width=10, fig.height=6.2)
options(knitr.kable.NA = '')
```


# Business Problem
Attrition is a problem that impacts all businesses, irrespective of geography, industry and size of the company. Employee attrition leads to significant costs for a business, including the cost of business disruption, hiring new staff and training new staff. As such, there is great business interest in understanding the drivers of, and minimizing staff attrition.

In this context, the use of classification models to predict if an employee is likely to quit could greatly increase the HR's ability to intervene on time and remedy the situation to prevent attrition. While this model can be routinely run to identify employees who are most likely to quit, the key driver of success would be the human element of reaching out the employee, understanding the current situation of the employee and taking action to remedy controllable factors that can prevent attrition of the employee.  

This data set presents an employee survey from IBM, indicating if there is attrition or not. The data set contains approximately 1500 entries. Given the limited size of the data set, the model should only be expected to provide modest improvement in indentification of attrition vs a random allocation of probability of attrition.

While some level of attrition in a company is inevitable, minimizing it and being prepared for the cases that cannot be helped will significantly help improve the operations of most businesses. As a future development, with a sufficiently large data set, it would be used to run a segmentation on employees, to develop certain "at risk" categories of employees. This could generate new insights for the business on what drives attrition, insights that cannot be generated by merely informational interviews with employees.


<hr>\clearpage

```{r echo=FALSE, message=FALSE}

# Please ENTER the filename
datafile_name = "../Data/WA_Fn-UseC_-HR-Employee-Attrition.csv"
IBMdata_DF<- read.csv(datafile_name)


str(IBMdata_DF)

# IBMdatafactor_scaled = apply(IBMdata, 2, function(r) {
#    if (sd(r) != 0) 
#        res = (r - mean(r))/sd(r) else res = 0 * r
#    res
#})

#fixing variables
IBMdata_DF$BusinessTravel<- as.factor(IBMdata_DF$BusinessTravel)
IBMdata_DF$Department <- as.factor(IBMdata_DF$Department)
IBMdata_DF$Education <- as.factor(IBMdata_DF$Education)
IBMdata_DF$Gender <- as.factor(IBMdata_DF$Gender)
IBMdata_DF$JobRole <- as.factor(IBMdata_DF$JobRole)
IBMdata_DF$MaritalStatus <- as.factor(IBMdata_DF$MaritalStatus)
IBMdata_DF$OverTime <- as.factor(IBMdata_DF$OverTime)
IBMdata_DF$EducationField <- as.factor(IBMdata_DF$EducationField)
IBMdata_DF$EmployeeCount <- as.factor(IBMdata_DF$EmployeeCount)
IBMdata_DF$EmployeeNumber <- as.factor(IBMdata_DF$EmployeeNumber)
IBMdata_DF$JobInvolvement <- as.factor(IBMdata_DF$JobInvolvement)
IBMdata_DF$JobLevel <- as.factor(IBMdata_DF$JobLevel)
IBMdata_DF$JobSatisfaction <- as.factor(IBMdata_DF$JobSatisfaction)
IBMdata_DF$Over18 <- as.factor(IBMdata_DF$Over18)
IBMdata_DF$PerformanceRating <- as.factor(IBMdata_DF$PerformanceRating)
IBMdata_DF$RelationshipSatisfaction <- as.factor(IBMdata_DF$RelationshipSatisfaction)
IBMdata_DF$StockOptionLevel <- as.factor(IBMdata_DF$StockOptionLevel)
IBMdata_DF$WorkLifeBalance <- as.factor(IBMdata_DF$WorkLifeBalance)


IBMdata <- data.matrix(IBMdata_DF)
dependent_variable = 2
independent_variables = c(1,3,8,11:21,23:26,28:35) # use all the available attributes
max_data_report = 10 
random_sampling = 1

dependent_variable = unique(sapply(dependent_variable,function(i) min(ncol(IBMdata), max(i,1))))
independent_variables = unique(sapply(independent_variables,function(i) min(ncol(IBMdata), max(i,1))))

if (length(unique(IBMdata[,dependent_variable])) !=2){
  cat("\n*****\n BE CAREFUL, THE DEPENDENT VARIABLE TAKES MORE THAN 2 VALUES")
  cat("\nSplitting it around its median...\n*****\n ")
  new_dependent = IBMdata[,dependent_variable] >= median(IBMdata[,dependent_variable])
  IBMdata[,dependent_variable] <- 1*new_dependent
}

# Please ENTER the probability threshold above which an observation is predicted as class 1:
Probability_Threshold = 0.45 # between 0 and 1

# Please ENTER the percentage of data used for estimation
estimation_data_percent = 80
validation_data_percent = 10
test_data_percent = 100-estimation_data_percent-validation_data_percent

# Tree parameter
# Please ENTER the tree (CART) complexity control cp (e.g. 0.0001 to 0.02, depending on the data)
CART_cp = 0.002
CART_control = rpart.control(cp = CART_cp)

# Please ENTER the words for the business interpretation of class 1 and class 0:
class_1_interpretation = "Attrition"
class_0_interpretation = "No Attrition"

```

<hr>\clearpage

# The Data
(Data source: https://www.kaggle.com/pavansubhasht/ibm-hr-analytics-attrition-dataset.)

IBM has gathered information on employee satisfaction, income, seniority and some demographics. It includes the data of 1470 employees. To use a matrix structure, we changed the model to reflect the followin data

#### Change the Table Below - CHECK TABLE 

Name                       | Description
:--------------------------|:--------------------------------------------------------------------
AGE                        | Numerical Value
ATTRITION                  | Employee leaving the company (0=no, 1=yes)
BUSINESS TRAVEL            | (1=No Travel, 2=Travel Frequently, 3=Tavel Rarely)
DAILY RATE                 | Numerical Value - Salary Level
DEPARTMENT                 | (1=HR, 2=R&D, 3=Sales)
DISTANCE FROM HOME         | Numerical Value - THE DISTANCE FROM WORK TO HOME 
EDUCATION                  | Numerical Value
EDUCATION FIELD            | (1=HR, 2=LIFE SCIENCES, 3=MARKETING, 4=MEDICAL SCIENCES, 5=OTHERS, 6= TEHCNICAL)
EMPLOYEE COUNT             | Numerical Value
EMPLOYEE NUMBER            | Numerical Value - EMPLOYEE ID 
ENVIROMENT SATISFACTION    | Numerical Value - SATISFACTION WITH THE ENVIROMENT 
GENDER                     | (1=FEMALE, 2=MALE)
HOURLY RATE                | Numerical Value - HOURLY SALARY 
JOB INVOLVEMENT            | Numerical Value - JOB INVOLVEMENT  
JOB LEVEL                  | Numerical Value - LEVEL OF JOB
JOB ROLE                   | (1=HC REP, 2=HR, 3=LAB TECHNICIAN, 4=MANAGER, 5= MANAGING DIRECTOR, 6= REASEARCH DIRECTOR, 7= RESEARCH SCIENTIST, 8=SALES EXECUTIEVE, 9= SALES REPRESENTATIVE) 
JOB SATISFACTION           | Numerical Value - SATISFACTION WITH THE JOB
MARITAL STATUS             | (1=DIVORCED, 2=MARRIED, 3=SINGLE)
MONTHLY INCOME             | Numerical Value - MONTHLY SALARY 
MONTHY RATE                | Numerical Value - MONTHY RATE
NUMCOMPANIES WORKED        | Numerical Value - NO. OF COMPANIES WORKED AT 
OVER 18                    | (1=YES, 2=NO)
OVERTIME                   | (1=NO, 2=YES)
PERCENT SALARY HIKE        | Numerical Value - PERCENTAGE INCREASE IN SALARY 
PERFORMANCE RATING         | Numerical Value - ERFORMANCE RATING    
RELATIONS SATISFACTION     | Numerical Value - RELATIONS SATISFACTION 
STANDARD HOURS             | Numerical Value - STANDARD HOURS 
STOCK OPTIONS LEVEL        | Numerical Value - STOCK OPTIONS  
TOTAL WORKING YEARS        | Numerical Value - TOTAL YEARS WORKED 
TRAINING TIMES LAST YEAR   | Numerical Value - HOURS SPENT TRAINING 
WORK LIFE BALANCE          | Numerical Value - TIME SPENT BEWTWEEN WORK AND OUTSIDE
YEARS AT COMPANY           | Numerical Value - TOTAL NUMBER OF YEARS AT THE COMPNAY 
YEARS IN CURRENT ROLE      | Numerical Value -YEARS IN CURRENT ROLE  
YEARS SINCE LAST PROMOTION | Numerical Value - LAST PROMOTION  
YEARS WITH CURRENT MANAGER | Numerical Value - YEARS SPENT WITH CURRENT MANAGER


# The Solution - Methodology

We plan to run a Logistic regression model and CART to determine the probability of a certain employee to fall into the condition of Attrition and thus its high risk of leaving the company. We will then test different parameters and probability threshold using confusion Matrixes, Area under the Curve and Gini Coefficient to determine which of the three models is the best predictor and will reccommend its use in practice.. 


Let's look into the data for a few customers. This is how the first `r min(max_data_report, nrow(IBMdata))` out of the total of `r nrow(IBMdata)` rows look like (transposed, for convenience):

```{r echo=FALSE, message=FALSE, prompt=FALSE, results='asis'}
knitr::kable({
  df <- t(head(round(IBMdata[,independent_variables],2), max_data_report))
  colnames(df) <- sprintf("%02d", 1:ncol(df))
  df
})
```
<hr>\clearpage

# The Process for Classification


1. Create an estimation sample and two validation samples by splitting the data into three groups. 
2. Set up the dependent variable, eployee attrition (as a categorical 0-1 variable)
3. Estimate the classification model using the estimation data, and interpret the results.
4. Assess the accuracy of classification in the first validation sample, possibly repeating steps 2-5 a few times changing the classifier in different ways to increase performance.
5. Finally, assess the accuracy of classification in the second validation sample. You should eventually use and report all relevant performance measures and plots on this second validation sample only.



## Step 1: Split the data 
We split the data into an estimation sample and two validation samples  - using a randomized splitting technique. The second validation data mimic out-of-sample data, and the performance on this validation set is a better approximation of the performance one should expect in practice from the selected classification method. The split used is 80% estimation, 10% validation, and 10% test data, depending on the number of observations - for example, when there is a lot of data, you may only keep a few hundreds of them for the validation and test sets, and use the rest for estimation. 


```{r echo=FALSE}
if (random_sampling){
  estimation_data_ids=sample.int(nrow(IBMdata),floor(estimation_data_percent*nrow(IBMdata)/100))
  non_estimation_data = setdiff(1:nrow(IBMdata),estimation_data_ids) #setdiff(x,y) returns the elements of x that are not in y
  validation_data_ids=non_estimation_data[sample.int(length(non_estimation_data), floor(validation_data_percent/(validation_data_percent+test_data_percent)*length(non_estimation_data)))]
  } else {
    estimation_data_ids=1:floor(estimation_data_percent*nrow(IBMdata)/100)
    non_estimation_data = setdiff(1:nrow(IBMdata),estimation_data_ids)
    validation_data_ids = (tail(estimation_data_ids,1)+1):(tail(estimation_data_ids,1) + floor(validation_data_percent/(validation_data_percent+test_data_percent)*length(non_estimation_data)))
    }

test_data_ids = setdiff(1:nrow(IBMdata), union(estimation_data_ids,validation_data_ids))

estimation_data=IBMdata[estimation_data_ids,]
validation_data=IBMdata[validation_data_ids,]
test_data=IBMdata[test_data_ids,]
```
<hr>\clearpage

## Step 2: Set up the dependent variable

The data original file was not organized as a categorical Valuable, so we changed the column "Attrition" to 0 and 1 values.

In our data the number of 0/1's in our estimation sample is as follows:

```{r echo=FALSE}
class_percentages=matrix(c(sum(estimation_data[,dependent_variable]==1),sum(estimation_data[,dependent_variable]==0)), nrow=1); colnames(class_percentages)<-c("Attrition", "No Attrition")
rownames(class_percentages)<-"# of Employees"
knitr::kable(class_percentages)
```

while in the validation sample they are:

```{r echo=FALSE}
class_percentages=matrix(c(sum(validation_data[,dependent_variable]==1),sum(validation_data[,dependent_variable]==0)), nrow=1); colnames(class_percentages)<-c("Attrition", "No Attrition")
rownames(class_percentages)<-"Employees"
knitr::kable(class_percentages)
```
<hr>\clearpage

## Step 3: Simple Analysis
We are running a simple table to visualize the Data of those values that are attrited

```{r echo=FALSE}
knitr::kable(round(my_summary(estimation_data[estimation_data[,dependent_variable]==1,independent_variables]),2))
```

And not attrited:

```{r echo=FALSE}
knitr::kable(round(my_summary(estimation_data[estimation_data[,dependent_variable]==0,independent_variables]),2))
```
<hr>\clearpage


## Step 4: Classification and Interpretation
Given our decisions, we decide to use a number of classification methods to develop a model that discriminates the different classes. 

In this paper we will consider: **logistic regression** and **classification and regression trees (CART)**. H 

**Logistic Regression**: Logistic Regression is a method similar to linear regression except that the dependent variable is discrete (e.g., 0 or 1). **Linear** logistic regression estimates the coefficients of a linear model using the selected independent variables while optimizing a classification criterion. For example, this is the logistic regression parameters for our data:


```{r echo=FALSE}
# We first turn the data into data.frame's
estimation_data = data.frame(estimation_data)
validation_data = data.frame(validation_data)
test_data = data.frame(test_data)

formula_log=paste(colnames(estimation_data[,dependent_variable,drop=F]),paste(Reduce(paste,sapply(head(independent_variables,-1), function(i) paste(colnames(estimation_data)[i],"+",sep=""))),colnames(estimation_data)[tail(independent_variables,1)],sep=""),sep="~") # When drop is FALSE, the dimensions of the object are kept. head(x,-1) returns all but the last element of x.

logreg_solution <- glm(formula_log, family=binomial(link="logit"),  data=estimation_data)
log_coefficients <- round(summary(logreg_solution)$coefficients,1)

knitr::kable(round(log_coefficients,2))
```

Given a set of independent variables, the output of the estimated logistic regression (the sum of the products of the independent variables with the corresponding regression coefficients) can be used to assess the probability an observation belongs to one of the classes. Specifically, the regression output can be transformed into a probability of belonging to, say, class 1 for each observation. The estimated probability that a validation observation belongs to class 1 (e.g., the estimated probability that the customer defaults) for the first few validation observations, using the logistic regression above, is:

```{r echo=FALSE}
# Let's get the probabilities for the 3 types of data from the logistic regression
estimation_Probability_class1_log<-predict(logreg_solution, type="response", newdata=estimation_data[,independent_variables])
validation_Probability_class1_log<-predict(logreg_solution, type="response", newdata=validation_data[,independent_variables])
test_Probability_class1_log<-predict(logreg_solution, type="response", newdata=test_data[,independent_variables])

# Let's get the decision of the logistic regression for the 3 types of data 
estimation_prediction_class_log=1*as.vector(estimation_Probability_class1_log > Probability_Threshold)
validation_prediction_class_log=1*as.vector(validation_Probability_class1_log > Probability_Threshold)
test_prediction_class_log=1*as.vector(test_Probability_class1_log > Probability_Threshold)

Classification_Table=rbind(validation_data[,dependent_variable],validation_prediction_class_log,validation_Probability_class1_log)
rownames(Classification_Table)<-c("Actual Class","Predicted Class","Probability of Class 1")
colnames(Classification_Table)<- paste("Obs", 1:ncol(Classification_Table), sep=" ")

knitr::kable(head(t(round(Classification_Table,2)), max_data_report)) #t(x) returns the transpose of x
```

The default decision is to classify each observation in the group with the highest probability.

**CART**

CART is a widely used classification method largely because the estimated classification models are easy to interpret. This classification tool iteratively "splits" the data using the most discriminatory independent variable at each step, building a "tree" - as shown below - on the way. The CART methods **limit the size of the tree** using various statistical techniques in order to avoid **overfitting the data**. For example, using the rpart and rpart.control functions in R, we can limit the size of the tree by selecting the functions' **complexity control** parameter **cp**. 

Running a basic CART model with complexity control cp=`r CART_cp`,  leads to the following tree (**NOTE**: for better readability of the tree figures below,  we will rename the independent variables as IV1 to `r paste("IV", length(independent_variables), sep="")` when using CART):

```{r echo=FALSE}
# Name the variables numerically so that they look ok on the tree plots
independent_variables_nolabel = paste("IV", 1:length(independent_variables), sep="")

estimation_data_nolabel = cbind(estimation_data[,dependent_variable], estimation_data[,independent_variables])
colnames(estimation_data_nolabel)<- c(colnames(estimation_data)[dependent_variable],independent_variables_nolabel)

validation_data_nolabel = cbind(validation_data[,dependent_variable], validation_data[,independent_variables])
colnames(validation_data_nolabel)<- c(dependent_variable,independent_variables_nolabel)

test_data_nolabel = cbind(test_data[,dependent_variable], test_data[,independent_variables])
colnames(test_data_nolabel)<- c(dependent_variable,independent_variables_nolabel)

estimation_data_nolabel = data.frame(estimation_data_nolabel)
validation_data_nolabel = data.frame(validation_data_nolabel)
test_data_nolabel = data.frame(test_data_nolabel)

formula=paste(colnames(estimation_data)[dependent_variable],paste(Reduce(paste,sapply(head(independent_variables_nolabel,-1), function(i) paste(i,"+",sep=""))),tail(independent_variables_nolabel,1),sep=""),sep="~")
CART_tree<-rpart(formula, data= estimation_data_nolabel,method="class", control=CART_control)

rpart.plot(CART_tree, box.palette="OrBu", type=3, extra=1, fallen.leaves=F, branch.lty=3)
```

The leaves of the tree indicate the number of estimation data observations that "reach that leaf" that belong to each class. A perfect classification would only have data from one class in each of the tree leaves. However, such a perfect classification of the estimation data would most likely not be able to classify well out-of-sample data due to overfitting of the estimation data.

```{r echo=FALSE}
# Tree parameter
# Please ENTER the new tree (CART) complexity control cp 
CART_cp = 0.0002
```

One can estimate larger trees through changing the tree's **complexity control** parameter (in this case the rpart.control argument cp). For example, this is how the tree would look like if we set cp=`r toString(CART_cp)`:

```{r echo=FALSE}
CART_tree_large<-rpart(formula, data= estimation_data_nolabel,method="class", control=rpart.control(cp = CART_cp))
rpart.plot(CART_tree_large, box.palette="OrBu", type=3, extra=1, fallen.leaves=F, branch.lty=3)
```

One can also use the percentage of data in each leaf of the tree to get an estimate of the probability that an observation (e.g., customer) belongs to a given class. The **purity of the leaf** can indicate the probability that an observation that "reaches that leaf" belongs to a class. In our case, the probability our validation data belong to class 1 (i.e., a customer's likelihood of default) for the first few validation observations, using the first CART above, is:

```{r echo=FALSE}
# Let's first calculate all probabilites for the estimation, validation, and test data
estimation_Probability_class1_tree<-predict(CART_tree, estimation_data_nolabel)[,2]
estimation_Probability_class1_tree_large<-predict(CART_tree_large, estimation_data_nolabel)[,2]

validation_Probability_class1_tree<-predict(CART_tree, validation_data_nolabel)[,2]
validation_Probability_class1_tree_large<-predict(CART_tree_large, validation_data_nolabel)[,2]

test_Probability_class1_tree<-predict(CART_tree, test_data_nolabel)[,2]
test_Probability_class1_tree_large<-predict(CART_tree_large, test_data_nolabel)[,2]

estimation_prediction_class_tree=1*as.vector(estimation_Probability_class1_tree > Probability_Threshold)
estimation_prediction_class_tree_large=1*as.vector(estimation_Probability_class1_tree_large > Probability_Threshold)

validation_prediction_class_tree=1*as.vector(validation_Probability_class1_tree > Probability_Threshold)
validation_prediction_class_tree_large=1*as.vector(validation_Probability_class1_tree_large > Probability_Threshold)

test_prediction_class_tree=1*as.vector(test_Probability_class1_tree > Probability_Threshold)
test_prediction_class_tree_large=1*as.vector(test_Probability_class1_tree_large > Probability_Threshold)

Classification_Table=rbind(validation_data[,dependent_variable],validation_prediction_class_tree,validation_Probability_class1_tree)
rownames(Classification_Table)<-c("Actual Class","Predicted Class","Probability of Class 1")
colnames(Classification_Table)<- paste("Obs", 1:ncol(Classification_Table), sep=" ")

Classification_Table_large=rbind(validation_data[,dependent_variable],validation_prediction_class_tree_large,validation_Probability_class1_tree_large)
rownames(Classification_Table_large)<-c("Actual Class","Predicted Class","Probability of Class 1")
colnames(Classification_Table_large)<- paste("Obs", 1:ncol(Classification_Table_large), sep=" ")

knitr::kable(head(t(round(Classification_Table,2)), max_data_report))
```

The table above assumes that the **probability threshold** for considering an observations as "class 1" is `r Probability_Threshold`. In practice we need to select the probability threshold: this is an important choice that we will discuss below.

 

```{r echo=FALSE, comment=NA, warning=FALSE, message=FALSE, results='asis'}
log_importance = tail(log_coefficients[,"z value", drop=F],-1) # remove the intercept
log_importance = log_importance/max(abs(log_importance))

tree_importance = CART_tree$variable.importance
tree_ordered_drivers = as.numeric(gsub("\\IV"," ",names(CART_tree$variable.importance)))
tree_importance_final = rep(0,length(independent_variables))
tree_importance_final[tree_ordered_drivers] <- tree_importance
tree_importance_final <- tree_importance_final/max(abs(tree_importance_final))
tree_importance_final <- tree_importance_final*sign(log_importance)

large_tree_importance = CART_tree_large$variable.importance
large_tree_ordered_drivers = as.numeric(gsub("\\IV"," ",names(CART_tree_large$variable.importance)))
large_tree_importance_final = rep(0,length(independent_variables))
large_tree_importance_final[large_tree_ordered_drivers] <- large_tree_importance
large_tree_importance_final <- large_tree_importance_final/max(abs(large_tree_importance_final))
large_tree_importance_final <- large_tree_importance_final*sign(log_importance)

Importance_table <- cbind(log_importance,tree_importance_final,large_tree_importance_final)
colnames(Importance_table) <- c("Logistic Regression", "CART 1", "CART 2")
rownames(Importance_table) <- rownames(log_importance)
## printing the result in a clean-slate table
knitr::kable(round(Importance_table,2))
```
<hr>\clearpage

## Step 5: Validation accuracy
Using the predicted class probabilities of the validation data, as outlined above, we can  generate some measures of classification performance.


### 1.  Hit ratio
This is the percentage of the observations that have been correctly classified (i.e., the predicted class and the actual class are the same). These are as follows for probability threshold `r Probability_Threshold*100`%:

```{r echo=FALSE}
validation_actual=validation_data[,dependent_variable]
validation_predictions = rbind(validation_prediction_class_log,
                               validation_prediction_class_tree,
                               validation_prediction_class_tree_large)
validation_hit_rates = rbind(
  100*sum(validation_prediction_class_log==validation_actual)/length(validation_actual),
  100*sum(validation_prediction_class_tree==validation_actual)/length(validation_actual), 
  100*sum(validation_prediction_class_tree_large==validation_actual)/length(validation_actual)
  )
colnames(validation_hit_rates) <- "Hit Ratio"
rownames(validation_hit_rates) <- c("Logistic Regression", "First CART", "Second CART")
knitr::kable(validation_hit_rates)
```

For the estimation data, the hit rates are:
```{r echo=FALSE}
estimation_actual=estimation_data[,dependent_variable]
estimation_predictions = rbind(estimation_prediction_class_log,
                               estimation_prediction_class_tree,
                               estimation_prediction_class_tree_large)
estimation_hit_rates = rbind(
  100*sum(estimation_prediction_class_log==estimation_actual)/length(estimation_actual),
  100*sum(estimation_prediction_class_tree==estimation_actual)/length(estimation_actual), 
  100*sum(estimation_prediction_class_tree_large==estimation_actual)/length(estimation_actual)
  )
colnames(estimation_hit_rates) <- "Hit Ratio"
rownames(estimation_hit_rates) <- c("Logistic Regression","First CART", "Second CART")
knitr::kable(estimation_hit_rates)
```


### 2. Confusion matrix
The confusion matrix shows for each class the number (or percentage) of the  data that are correctly classified for that class. For example, for the method above with the highest hit rate in the validation data (among logistic regression and the 2 CART models), and for probability threshold `r Probability_Threshold*100`%, the confusion matrix for the validation data is:

```{r echo=FALSE}
validation_prediction_best = validation_predictions[which.max(validation_hit_rates),]
conf_matrix = matrix(rep(0,4),ncol=2)
conf_matrix[1,1]<- 100*sum(validation_prediction_best*validation_data[,dependent_variable])/sum(validation_data[,dependent_variable])
conf_matrix[1,2]<- 100*sum((!validation_prediction_best)*validation_data[,dependent_variable])/sum(validation_data[,dependent_variable])
conf_matrix[2,1]<- 100*sum((validation_prediction_best)*(!validation_data[,dependent_variable]))/sum((!validation_data[,dependent_variable]))
conf_matrix[2,2]<- 100*sum((!validation_prediction_best)*(!validation_data[,dependent_variable]))/sum((!validation_data[,dependent_variable]))
conf_matrix = round(conf_matrix,2)

colnames(conf_matrix) <- c(paste("Predicted 1 (", class_1_interpretation, ")", sep = ""), paste("Predicted 0 (", class_0_interpretation, ")", sep = ""))
rownames(conf_matrix) <- c(paste("Actual 1 (", class_1_interpretation, ")", sep = ""), paste("Actual 0 (", class_0_interpretation, ")", sep = ""))
knitr::kable(conf_matrix)
```



### 3. ROC curve

The ROC curves for the validation data for the logistic regression as well as both the CARTs above are as follows:

```{r echo=FALSE}
validation_actual_class <- as.numeric(validation_data[,dependent_variable])

pred_tree <- prediction(validation_Probability_class1_tree, validation_actual_class)
pred_tree_large <- prediction(validation_Probability_class1_tree_large, validation_actual_class)
pred_log <- prediction(validation_Probability_class1_log, validation_actual_class)

test1<-performance(pred_tree, "tpr", "fpr")
df1<- cbind(as.data.frame(test1@x.values),as.data.frame(test1@y.values))
colnames(df1) <- c("False Positive rate CART 1", "True Positive rate CART 1")
plot1 <- ggplot(df1, aes(x=`False Positive rate CART 1`, y=`True Positive rate CART 1`)) + geom_line()
test2<-performance(pred_log, "tpr", "fpr")
df2<- cbind(as.data.frame(test2@x.values),as.data.frame(test2@y.values))
colnames(df2) <- c("False Positive rate log reg", "True Positive rate log reg")
plot2 <- ggplot(df2, aes(x=`False Positive rate log reg`, y=`True Positive rate log reg`)) + geom_line()
test3<-performance(pred_tree_large, "tpr", "fpr")
df3<- cbind(as.data.frame(test3@x.values),as.data.frame(test3@y.values))
colnames(df3) <- c("False Positive rate CART 2", "True Positive rate CART 2")
plot3 <- ggplot(df3, aes(x=`False Positive rate CART 2`, y=`True Positive rate CART 2`)) + geom_line()

# We can plot the curves individually 
# grid.arrange(plot1, plot2, plot3)   # use `fig.height=7.5` for the grid plot

# But we are going to combine them instead
df.all <- do.call(rbind, lapply(list(df1, df2, df3), function(df) {
  df <- melt(df, id=1)
  df$variable <- sub("True Positive rate ", "", df$variable)
  colnames(df)[1] <- "False Positive rate"
  df
}))
ggplot(df.all, aes(x=`False Positive rate`, y=value, colour=variable)) + geom_line() + ylab("True Positive rate") + geom_abline(intercept = 0, slope = 1,linetype="dotted",colour="green")
```


### 4. Gains chart

The gains charts for the validation data for our three classifiers are the following:

```{r echo=FALSE}
validation_actual <- validation_data[,dependent_variable]
all1s <- sum(validation_actual); 

probs <- validation_Probability_class1_tree
xaxis <- sort(unique(c(0,1,probs)), decreasing = TRUE)
res <- 100*Reduce(cbind,lapply(xaxis, function(prob){
  useonly <- which(probs >= prob)
  c(length(useonly)/length(validation_actual), sum(validation_actual[useonly])/all1s) 
}))
frame1 <- data.frame(
  `CART 1 % of validation data` = res[1,],
  `CART 1 % of class 1` = res[2,],
  check.names = FALSE
)
plot1 <- ggplot(frame1, aes(x=`CART 1 % of validation data`, y=`CART 1 % of class 1`)) + geom_line()

probs <- validation_Probability_class1_tree_large
xaxis <- sort(unique(c(0,1,probs)), decreasing = TRUE)
res <- 100*Reduce(cbind,lapply(xaxis, function(prob){
  useonly <- which(probs >= prob)
  c(length(useonly)/length(validation_actual), sum(validation_actual[useonly])/all1s) 
}))
frame2 <- data.frame(
  `CART 2 % of validation data` = res[1,],
  `CART 2 % of class 1` = res[2,],
  check.names = FALSE
)
plot2 <- ggplot(frame2, aes(x=`CART 2 % of validation data`, y=`CART 2 % of class 1`)) + geom_line()

probs <- validation_Probability_class1_log
xaxis <- sort(unique(c(0,1,probs)), decreasing = TRUE)
res <- 100*Reduce(cbind,lapply(xaxis, function(prob){
  useonly <- which(probs >= prob)
  c(length(useonly)/length(validation_actual), sum(validation_actual[useonly])/all1s) 
}))
frame3 <- data.frame(
  `log reg % of validation data` = res[1,],
  `log reg % of class 1` = res[2,],
  check.names = FALSE
)
plot3 <- ggplot(frame3, aes(x=`log reg % of validation data`, y=`log reg % of class 1`)) + geom_line()

# We can plot the curves individually
# grid.arrange(plot1, plot2, plot3)   # use `fig.height=7.5` for the grid plot

# But we are going to combine them instead
df.all <- do.call(rbind, lapply(list(frame1, frame2, frame3), function(df) {
  df <- melt(df, id=1)
  df$variable <- sub(" % of class 1", "", df$variable)
  colnames(df)[1] <- "% of validation data selected"
  df
}))
ggplot(df.all, aes(x=`% of validation data selected`, y=value, colour=variable)) + geom_line() + ylab("% of class 1 captured") + geom_abline(intercept = 0, slope = 1,linetype="dotted",colour="green")
```
<hr>\clearpage

## Step 6. Test Accuracy
Having iterated steps 2-5 until we are satisfied with the performance of our selected model on the validation data, in this step the performance analysis outlined in step 5 needs to be done with the test sample. ** 

Let's see in our case how the **hit ratio, confusion matrix, ROC curve, gains chart, and profit curve** look like for our test data. For the hit ratio and the confusion matrix we use `r Probability_Threshold*100`% as the probability threshold for classification.

```{r echo=FALSE}
######for train data#####
test_actual=test_data[,dependent_variable]
test_predictions = rbind(test_prediction_class_log,
                         test_prediction_class_tree,
                         test_prediction_class_tree_large
                         )
test_hit_rates = rbind(
  100*sum(test_prediction_class_log==test_actual)/length(test_actual),
  100*sum(test_prediction_class_tree==test_actual)/length(test_actual), 
  100*sum(test_prediction_class_tree_large==test_actual)/length(test_actual)
  )
colnames(test_hit_rates) <- "Hit Ratio"
rownames(test_hit_rates) <- c("Logistic Regression","First CART", "Second CART")

knitr::kable(test_hit_rates)
```

The confusion matrix for the model with the best test data hit ratio above:

```{r echo=FALSE, message=FALSE, prompt=FALSE, results='asis'}
test_prediction_best = test_predictions[which.max(test_hit_rates),]
conf_matrix = matrix(rep(0,4),ncol=2)
conf_matrix[1,1]<- 100*sum(test_prediction_best*test_data[,dependent_variable])/sum(test_data[,dependent_variable])
conf_matrix[1,2]<- 100*sum((!test_prediction_best)*test_data[,dependent_variable])/sum(test_data[,dependent_variable])
conf_matrix[2,1]<- 100*sum((test_prediction_best)*(!test_data[,dependent_variable]))/sum((!test_data[,dependent_variable]))
conf_matrix[2,2]<- 100*sum((!test_prediction_best)*(!test_data[,dependent_variable]))/sum((!test_data[,dependent_variable]))
conf_matrix = round(conf_matrix,2)

colnames(conf_matrix) <- c(paste("Predicted 1 (", class_1_interpretation, ")", sep = ""), paste("Predicted 0 (", class_0_interpretation, ")", sep = ""))
rownames(conf_matrix) <- c(paste("Actual 1 (", class_1_interpretation, ")", sep = ""), paste("Actual 0 (", class_0_interpretation, ")", sep = ""))
knitr::kable(conf_matrix)
```

ROC curves for the test data:

```{r echo=FALSE}
test_actual_class <- as.numeric(test_data[,dependent_variable])

pred_tree_test <- prediction(test_Probability_class1_tree, test_actual_class)
pred_tree_large_test <- prediction(test_Probability_class1_tree_large, test_actual_class)
pred_log_test <- prediction(test_Probability_class1_log, test_actual_class)

test<-performance(pred_tree_test, "tpr", "fpr")
df1<- cbind(as.data.frame(test@x.values),as.data.frame(test@y.values))
colnames(df1) <- c("False Positive rate CART 1", "True Positive CART 1")
test2<-performance(pred_tree_large_test, "tpr", "fpr")
df2<- cbind(as.data.frame(test2@x.values),as.data.frame(test2@y.values))
colnames(df2) <- c("False Positive rate CART 2", "True Positive CART 2")
test3<-performance(pred_log_test, "tpr", "fpr")
df3<- cbind(as.data.frame(test3@x.values),as.data.frame(test3@y.values))
colnames(df3) <- c("False Positive rate log reg", "True Positive log reg")

df.all <- do.call(rbind, lapply(list(df1, df2, df3), function(df) {
  df <- melt(df, id=1)
  df$variable <- sub("True Positive ", "", df$variable)
  colnames(df)[1] <- "False Positive rate"
  df
}))
ggplot(df.all, aes(x=`False Positive rate`, y=value, colour=variable)) + geom_line() + ylab("True Positive rate") + geom_abline(intercept = 0, slope = 1,linetype="dotted",colour="green")
```

Gains chart for the test data:

```{r echo=FALSE}
test_actual <- test_data[,dependent_variable]
all1s <- sum(test_actual)

probs <- test_Probability_class1_tree
xaxis <- sort(unique(c(0,1,probs)), decreasing = TRUE)
res <- 100*Reduce(cbind,lapply(xaxis, function(prob){
  useonly <- which(probs >= prob)
  c(length(useonly)/length(test_actual), sum(test_actual[useonly])/all1s) 
}))
frame1 <- data.frame(
  `CART 1 % of validation data` = res[1,],
  `CART 1 % of class 1` = res[2,],
  check.names = FALSE
)

probs <- test_Probability_class1_tree_large
xaxis <- sort(unique(c(0,1,probs)), decreasing = TRUE)
res <- 100*Reduce(cbind,lapply(xaxis, function(prob){
  useonly <- which(probs >= prob)
  c(length(useonly)/length(test_actual), sum(test_actual[useonly])/all1s) 
}))
frame2 <- data.frame(
  `CART 2 % of validation data` = res[1,],
  `CART 2 % of class 1` = res[2,],
  check.names = FALSE
)

probs <- test_Probability_class1_log
xaxis <- sort(unique(c(0,1,probs)), decreasing = TRUE)
res <- 100*Reduce(cbind,lapply(xaxis, function(prob){
  useonly <- which(probs >= prob)
  c(length(useonly)/length(test_actual), sum(test_actual[useonly])/all1s) 
  }))
frame3 <- data.frame(
  `log reg % of validation data` = res[1,],
  `log reg % of class 1` = res[2,],
  check.names = FALSE
)

df.all <- do.call(rbind, lapply(list(frame1, frame2, frame3), function(df) {
  df <- melt(df, id=1)
  df$variable <- sub(" % of class 1", "", df$variable)
  colnames(df)[1] <- "% of test data selected"
  df
}))
ggplot(df.all, aes(x=`% of test data selected`, y=value, colour=variable)) + geom_line() + ylab("% of class 1 captured") + geom_abline(intercept = 0, slope = 1,linetype="dotted",colour="green")
```
<hr>\clearpage

## Step 7. Data Analysis

After we ran the model multiple times and iterate to find the best value, we came with some conclusions:

 - Model is biased towards predicting non attrition. 
 - There is a tension between probability threshold and the number of employees who are accurately predicted as potential churners. A high probability threshold would end in a high number of errors. The business relevance is predict attrition well, rather than non attrition hence a lower probability threshold is chosen.
 - The confusion matrix shows that of all the people who are going to leave the company, our algorithm identifies about 42% of them accurately. While not ideal, this is a huge improvement on random sampling where we could have predicted only about 16% (the actual attrition rate). On the other hand, there is a cost of wrongly identifying attrition of non-leaving employees resulting in inefficiencies in resource allocation.
  - Log Regression is the best model, as it always predict a higher area under the curve and a better confusion matrix
